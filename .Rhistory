mutate(CounterID = "Lighthouse")
bayside_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `Bayside Pedestrian In`,
PedestriansOut = `Bayside Pedestrian out`) %>%
mutate(CounterID = "Bayside Trail")
lot1_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `CABR Lot 1 Counter IN`,
PedestriansOut = `CABR Lot 1 Counter OUT`) %>%
mutate(CounterID = "Tidepool Lot 1")
lot2_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `CABR Lot 2 Counter IN`,
PedestriansOut = `CABR Lot 2 Counter OUT`) %>%
mutate(CounterID = "Tidepool Lot 2")
Oceanside1_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `Oceanside Trail 1 Pedestrian IN`,
PedestriansOut = `Oceanside Trail 1 Pedestrian OUT`) %>%
mutate(CounterID = "Oceanside Trail 1")
# Combine all sites
processed_data <- bind_rows(lighthouse_data, bayside_data, lot1_data,
lot2_data, Oceanside1_data) %>%
mutate(
TotalCount = case_when(
is.na(PedestriansIn) & is.na(PedestriansOut) ~ NA_real_,
is.na(PedestriansIn) ~ PedestriansOut,
is.na(PedestriansOut) ~ PedestriansIn,
TRUE ~ PedestriansIn + PedestriansOut
)
) %>%
select(
CounterID,
DateTime,
Date,
Hour,
Year,
Month,
Day,
Weekday,
PedestriansIn,
PedestriansOut,
TotalCount
) %>%
arrange(CounterID, DateTime)
return(processed_data)
}
# Function to create Power BI optimized datasets
create_powerbi_datasets <- function(data) {
# 1. Main dataset - cleaned and formatted for Power BI
powerbi_main <- data %>%
filter(!is.na(TotalCount)) %>%
mutate(
# Create Power BI friendly date formats
DateKey = as.numeric(format(Date, "%Y%m%d")),
MonthYear = format(Date, "%Y-%m"),
MonthName = month(Date, label = TRUE, abbr = FALSE),
DayOfWeek = wday(Date, label = TRUE, abbr = FALSE),
Quarter = paste0("Q", quarter(Date)),
WeekNumber = week(Date),
IsWeekend = ifelse(wday(Date) %in% c(1, 7), "Weekend", "Weekday"),
TimeOfDay = case_when(
Hour >= 5 & Hour < 12 ~ "Morning",
Hour >= 12 & Hour < 17 ~ "Afternoon",
Hour >= 17 & Hour < 21 ~ "Evening",
TRUE ~ "Night"
),
# Create descriptive trail categories
TrailCategory = case_when(
grepl("Lighthouse", CounterID) ~ "Lighthouse Area",
grepl("Bayside", CounterID) ~ "Bayside Trail",
grepl("Tidepool", CounterID) ~ "Tidepool Area",
grepl("Oceanside", CounterID) ~ "Oceanside Trail",
TRUE ~ "Other"
)
) %>%
# Ensure no missing values in key fields
filter(!is.na(PedestriansIn) | !is.na(PedestriansOut)) %>%
# Replace NA with 0 for calculations
mutate(
PedestriansIn = ifelse(is.na(PedestriansIn), 0, PedestriansIn),
PedestriansOut = ifelse(is.na(PedestriansOut), 0, PedestriansOut),
TotalCount = PedestriansIn + PedestriansOut
)
# 2. Daily summary for dashboard KPIs
daily_summary <- powerbi_main %>%
group_by(CounterID, TrailCategory, Date, DateKey, MonthYear,
MonthName, DayOfWeek, IsWeekend, Quarter) %>%
summarise(
DailyTotal = sum(TotalCount, na.rm = TRUE),
DailyIn = sum(PedestriansIn, na.rm = TRUE),
DailyOut = sum(PedestriansOut, na.rm = TRUE),
PeakHour = Hour[which.max(TotalCount)],
PeakHourCount = max(TotalCount, na.rm = TRUE),
ActiveHours = sum(TotalCount > 0, na.rm = TRUE),
.groups = 'drop'
)
# 3. Monthly summary for trend analysis
monthly_summary <- powerbi_main %>%
group_by(CounterID, TrailCategory, MonthYear, Year, Month, MonthName, Quarter) %>%
summarise(
MonthlyTotal = sum(TotalCount, na.rm = TRUE),
MonthlyAvgDaily = round(mean(TotalCount, na.rm = TRUE), 1),
MonthlyPeak = max(TotalCount, na.rm = TRUE),
DaysActive = n_distinct(Date),
.groups = 'drop'
) %>%
arrange(CounterID, Year, Month)
# 4. Hour of day patterns
hourly_patterns <- powerbi_main %>%
group_by(CounterID, TrailCategory, Hour, TimeOfDay) %>%
summarise(
AvgHourlyCount = round(mean(TotalCount, na.rm = TRUE), 1),
MedianHourlyCount = round(median(TotalCount, na.rm = TRUE), 1),
MaxHourlyCount = max(TotalCount, na.rm = TRUE),
TotalObservations = n(),
.groups = 'drop'
)
# 5. Trail comparison summary
trail_comparison <- powerbi_main %>%
group_by(CounterID, TrailCategory) %>%
summarise(
TotalVisitors = sum(TotalCount, na.rm = TRUE),
AvgDailyVisitors = round(mean(TotalCount, na.rm = TRUE), 1),
PeakSingleHour = max(TotalCount, na.rm = TRUE),
DaysOfData = n_distinct(Date),
FirstRecordDate = min(Date),
LastRecordDate = max(Date),
.groups = 'drop'
) %>%
arrange(desc(TotalVisitors))
# Return list of datasets
return(list(
main = powerbi_main,
daily = daily_summary,
monthly = monthly_summary,
hourly = hourly_patterns,
comparison = trail_comparison
))
}
# Function to export Power BI datasets
export_powerbi_data <- function(datasets, base_path = "Data/Processed/PowerBI/") {
# Create directory if it doesn't exist
if (!dir.exists(base_path)) {
dir.create(base_path, recursive = TRUE)
}
# Export each dataset
write_csv(datasets$main, paste0(base_path, "EcoCounter_PowerBI_Main.csv"))
write_csv(datasets$daily, paste0(base_path, "EcoCounter_PowerBI_Daily.csv"))
write_csv(datasets$monthly, paste0(base_path, "EcoCounter_PowerBI_Monthly.csv"))
write_csv(datasets$hourly, paste0(base_path, "EcoCounter_PowerBI_Hourly.csv"))
write_csv(datasets$comparison, paste0(base_path, "EcoCounter_PowerBI_Comparison.csv"))
cat("\n=== POWER BI DATASETS EXPORTED ===\n")
cat("Main dataset:", nrow(datasets$main), "records\n")
cat("Daily summary:", nrow(datasets$daily), "records\n")
cat("Monthly summary:", nrow(datasets$monthly), "records\n")
cat("Hourly patterns:", nrow(datasets$hourly), "records\n")
cat("Trail comparison:", nrow(datasets$comparison), "records\n")
cat("\nFiles saved to:", base_path, "\n")
# Print dataset preview
cat("\n=== SAMPLE OF MAIN DATASET ===\n")
print(head(datasets$main, 5))
return(datasets)
}
# Main execution
processed_data <- process_ecocounter_data(file_path)
# Create Power BI optimized datasets
powerbi_datasets <- create_powerbi_datasets(processed_data)
# Export for Power BI
final_powerbi_data <- export_powerbi_data(powerbi_datasets)
# Keep your existing exports
final_data <- export_for_arcgis(processed_data)
# Create summary statistics (your existing code)
Counter_summary_by_date <- processed_data %>%
na.exclude() %>%
filter(Hour >= 9 & Hour <= 17) %>%
group_by(CounterID, Date) %>%
summarise(sum_by_date = sum(PedestriansOut))
max_date <- Counter_summary_by_date %>%
group_by(CounterID) %>%
summarise(max_value = max(sum_by_date),
max_date = Date[which.max(sum_by_date)])
weekly_summary <- Counter_summary_by_date %>%
mutate(
Week = floor_date(Date, unit = "week", week_start = 1)
) %>%
group_by(CounterID, Week) %>%
summarise(
WeeklyTotal = sum(sum_by_date),
.groups = 'drop'
)
monthly_summary <- Counter_summary_by_date %>%
mutate(
month = floor_date(Date, unit = "month")
) %>%
group_by(CounterID, month) %>%
summarise(
MonthlyTotal = sum(sum_by_date),
.groups = 'drop'
)
# Export existing summaries
ecocounter_no_NA <- final_data %>% na.exclude()
#write_csv(ecocounter_no_NA, file = "Data/Processed/Eco_Counter_no_NA.csv")
write_csv(max_date, file = "Data/Processed/Max_Daily.csv")
write_csv(weekly_summary, file = "Data/Processed/Max_Weekly.csv")
write_csv(monthly_summary, file = "Data/Processed/Max_Monthly.csv")
cat("\nProcessing complete! All files ready for Power BI and ArcGIS Online upload.\n")
cat("Power BI files created in: Data/Processed/PowerBI/\n")
cat("Data for", length(unique(processed_data$CounterID)), "sites processed successfully.\n")
# Load required libraries
library(dplyr)
library(lubridate)
library(readr)
# Set file path at the top
file_path <- "C:/Users/tkatayama/OneDrive - DOI/Documents/Projects/R/People_Counter/Data/Raw/All_20250507_0622.csv"
# Function to process eco-counter data for multiple sites
process_ecocounter_data <- function(file_path) {
# Read CSV file, skipping first 2 rows
raw_data <- read_csv(file_path, skip = 2, na = c(""))
# Display original column names for verification
cat("Original column names:\n")
print(names(raw_data))
# First, clean and prepare the base data
clean_data <- raw_data %>%
filter(!is.na(Time)) %>%
mutate(
DateTime = mdy_hm(Time),
DateTime = case_when(
is.na(DateTime) ~ mdy_hms(paste0(Time, ":00")),
TRUE ~ DateTime
)
) %>%
filter(!is.na(DateTime)) %>%
mutate(
Date = as.Date(DateTime),
Hour = hour(DateTime),
Year = year(DateTime),
Month = month(DateTime),
Day = day(DateTime),
Weekday = wday(DateTime, label = TRUE)
)
# Create separate dataframes for each site
lighthouse_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `Lighthouse Pedestrian towards lighthouse`,
PedestriansOut = `Lighthouse Pedestrian out of lighthouse`) %>%
mutate(CounterID = "Lighthouse")
bayside_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `Bayside Pedestrian In`,
PedestriansOut = `Bayside Pedestrian out`) %>%
mutate(CounterID = "Bayside Trail")
lot1_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `CABR Lot 1 Counter IN`,
PedestriansOut = `CABR Lot 1 Counter OUT`) %>%
mutate(CounterID = "Tidepool Lot 1")
lot2_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `CABR Lot 2 Counter IN`,
PedestriansOut = `CABR Lot 2 Counter OUT`) %>%
mutate(CounterID = "Tidepool Lot 2")
Oceanside1_data <- clean_data %>%
select(DateTime, Date, Hour, Year, Month, Day, Weekday,
PedestriansIn = `Oceanside Trail 1 Pedestrian IN`,
PedestriansOut = `Oceanside Trail 1 Pedestrian OUT`) %>%
mutate(CounterID = "Oceanside Trail 1")
# Combine all sites
processed_data <- bind_rows(lighthouse_data, bayside_data, lot1_data,
lot2_data, Oceanside1_data) %>%
mutate(
# Use only OUT count to avoid double counting (represents unique visitors leaving)
# If OUT is missing, use IN as backup
TotalCount = case_when(
is.na(PedestriansOut) & is.na(PedestriansIn) ~ NA_real_,
is.na(PedestriansOut) ~ PedestriansIn,
TRUE ~ PedestriansOut
),
# Create a combined count for reference (but not used in main calculations)
CombinedCount = case_when(
is.na(PedestriansIn) & is.na(PedestriansOut) ~ NA_real_,
is.na(PedestriansIn) ~ PedestriansOut,
is.na(PedestriansOut) ~ PedestriansIn,
TRUE ~ PedestriansIn + PedestriansOut
)
) %>%
select(
CounterID,
DateTime,
Date,
Hour,
Year,
Month,
Day,
Weekday,
PedestriansIn,
PedestriansOut,
TotalCount
) %>%
arrange(CounterID, DateTime)
return(processed_data)
}
# Function to create Power BI optimized datasets
create_powerbi_datasets <- function(data) {
# 1. Main dataset - cleaned and formatted for Power BI
powerbi_main <- data %>%
filter(!is.na(TotalCount)) %>%
mutate(
# Create Power BI friendly date formats
DateKey = as.numeric(format(Date, "%Y%m%d")),
MonthYear = format(Date, "%Y-%m"),
MonthName = month(Date, label = TRUE, abbr = FALSE),
DayOfWeek = wday(Date, label = TRUE, abbr = FALSE),
Quarter = paste0("Q", quarter(Date)),
WeekNumber = week(Date),
IsWeekend = ifelse(wday(Date) %in% c(1, 7), "Weekend", "Weekday"),
TimeOfDay = case_when(
Hour >= 5 & Hour < 12 ~ "Morning",
Hour >= 12 & Hour < 17 ~ "Afternoon",
Hour >= 17 & Hour < 21 ~ "Evening",
TRUE ~ "Night"
),
# Create descriptive trail categories
TrailCategory = case_when(
grepl("Lighthouse", CounterID) ~ "Lighthouse Area",
grepl("Bayside", CounterID) ~ "Bayside Trail",
grepl("Tidepool", CounterID) ~ "Tidepool Area",
grepl("Oceanside", CounterID) ~ "Oceanside Trail",
TRUE ~ "Other"
)
) %>%
# Ensure no missing values in key fields
filter(!is.na(PedestriansIn) | !is.na(PedestriansOut)) %>%
# Use OUT count as primary measure to avoid double counting
# Replace NA with 0 only where needed
mutate(
PedestriansIn = ifelse(is.na(PedestriansIn), 0, PedestriansIn),
PedestriansOut = ifelse(is.na(PedestriansOut), 0, PedestriansOut),
# Primary count uses OUT (represents unique visitors)
TotalCount = case_when(
PedestriansOut > 0 ~ PedestriansOut,
PedestriansOut == 0 & PedestriansIn > 0 ~ PedestriansIn,
TRUE ~ 0
),
# Keep combined for reference
CombinedCount = PedestriansIn + PedestriansOut
)
# 2. Daily summary for dashboard KPIs
daily_summary <- powerbi_main %>%
group_by(CounterID, TrailCategory, Date, DateKey, MonthYear,
MonthName, DayOfWeek, IsWeekend, Quarter) %>%
summarise(
DailyTotal = sum(TotalCount, na.rm = TRUE),  # Uses OUT count only
DailyIn = sum(PedestriansIn, na.rm = TRUE),
DailyOut = sum(PedestriansOut, na.rm = TRUE),
DailyCombined = sum(CombinedCount, na.rm = TRUE),  # For reference
PeakHour = Hour[which.max(TotalCount)],  # Hour of peak OUT count
PeakHourCount = max(TotalCount, na.rm = TRUE),  # Peak OUT count
ActiveHours = sum(TotalCount > 0, na.rm = TRUE),
.groups = 'drop'
)
# 3. Monthly summary for trend analysis
monthly_summary <- powerbi_main %>%
group_by(CounterID, TrailCategory, MonthYear, Year, Month, MonthName, Quarter) %>%
summarise(
MonthlyTotal = sum(TotalCount, na.rm = TRUE),
MonthlyAvgDaily = round(mean(TotalCount, na.rm = TRUE), 1),
MonthlyPeak = max(TotalCount, na.rm = TRUE),
DaysActive = n_distinct(Date),
.groups = 'drop'
) %>%
arrange(CounterID, Year, Month)
# 4. Hour of day patterns
hourly_patterns <- powerbi_main %>%
group_by(CounterID, TrailCategory, Hour, TimeOfDay) %>%
summarise(
AvgHourlyCount = round(mean(TotalCount, na.rm = TRUE), 1),
MedianHourlyCount = round(median(TotalCount, na.rm = TRUE), 1),
MaxHourlyCount = max(TotalCount, na.rm = TRUE),
TotalObservations = n(),
.groups = 'drop'
)
# 5. Trail comparison summary
trail_comparison <- powerbi_main %>%
group_by(CounterID, TrailCategory) %>%
summarise(
TotalVisitors = sum(TotalCount, na.rm = TRUE),
AvgDailyVisitors = round(mean(TotalCount, na.rm = TRUE), 1),
PeakSingleHour = max(TotalCount, na.rm = TRUE),
DaysOfData = n_distinct(Date),
FirstRecordDate = min(Date),
LastRecordDate = max(Date),
.groups = 'drop'
) %>%
arrange(desc(TotalVisitors))
# Return list of datasets
return(list(
main = powerbi_main,
daily = daily_summary,
monthly = monthly_summary,
hourly = hourly_patterns,
comparison = trail_comparison
))
}
# Function to export Power BI datasets with automatic refresh setup
export_powerbi_data <- function(datasets, base_path = "Data/Processed/PowerBI/") {
# Create directory if it doesn't exist
if (!dir.exists(base_path)) {
dir.create(base_path, recursive = TRUE)
}
# Add timestamp for tracking updates
timestamp <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
# Add metadata row to each dataset for Power BI refresh tracking
datasets$main$LastUpdated <- timestamp
datasets$daily$LastUpdated <- timestamp
datasets$monthly$LastUpdated <- timestamp
datasets$hourly$LastUpdated <- timestamp
datasets$comparison$LastUpdated <- timestamp
# Export each dataset with consistent naming for Power BI auto-refresh
write_csv(datasets$main, paste0(base_path, "EcoCounter_PowerBI_Main.csv"))
write_csv(datasets$daily, paste0(base_path, "EcoCounter_PowerBI_Daily.csv"))
write_csv(datasets$monthly, paste0(base_path, "EcoCounter_PowerBI_Monthly.csv"))
write_csv(datasets$hourly, paste0(base_path, "EcoCounter_PowerBI_Hourly.csv"))
write_csv(datasets$comparison, paste0(base_path, "EcoCounter_PowerBI_Comparison.csv"))
# Create a refresh log file
refresh_log <- data.frame(
RefreshDate = timestamp,
MainRecords = nrow(datasets$main),
DailyRecords = nrow(datasets$daily),
MonthlyRecords = nrow(datasets$monthly),
HourlyRecords = nrow(datasets$hourly),
ComparisonRecords = nrow(datasets$comparison),
DateRange_Start = min(datasets$main$Date),
DateRange_End = max(datasets$main$Date),
TrailsIncluded = length(unique(datasets$main$CounterID))
)
# Append to log (create if doesn't exist)
log_file <- paste0(base_path, "PowerBI_Refresh_Log.csv")
if (file.exists(log_file)) {
existing_log <- read_csv(log_file, show_col_types = FALSE)
refresh_log <- bind_rows(existing_log, refresh_log)
}
write_csv(refresh_log, log_file)
cat("\n=== POWER BI DATASETS EXPORTED (NO DOUBLE COUNTING) ===\n")
cat("Last Updated:", timestamp, "\n")
cat("Main dataset:", nrow(datasets$main), "records\n")
cat("Daily summary:", nrow(datasets$daily), "records\n")
cat("Monthly summary:", nrow(datasets$monthly), "records\n")
cat("Hourly patterns:", nrow(datasets$hourly), "records\n")
cat("Trail comparison:", nrow(datasets$comparison), "records\n")
cat("Date range:", min(datasets$main$Date), "to", max(datasets$main$Date), "\n")
cat("\nFiles saved to:", base_path, "\n")
cat("NOTE: TotalCount uses OUT count only to avoid double counting visitors\n")
# Print dataset preview
cat("\n=== SAMPLE OF MAIN DATASET ===\n")
sample_data <- datasets$main %>%
select(CounterID, Date, Hour, TotalCount, PedestriansOut, CombinedCount, LastUpdated) %>%
head(5)
print(sample_data)
return(datasets)
}
# Main execution
processed_data <- process_ecocounter_data(file_path)
# Create Power BI optimized datasets
powerbi_datasets <- create_powerbi_datasets(processed_data)
# Export for Power BI
final_powerbi_data <- export_powerbi_data(powerbi_datasets)
# Keep your existing exports
final_data <- export_for_arcgis(processed_data)
# Create summary statistics (your existing code)
Counter_summary_by_date <- processed_data %>%
na.exclude() %>%
filter(Hour >= 9 & Hour <= 17) %>%
group_by(CounterID, Date) %>%
summarise(sum_by_date = sum(PedestriansOut))
max_date <- Counter_summary_by_date %>%
group_by(CounterID) %>%
summarise(max_value = max(sum_by_date),
max_date = Date[which.max(sum_by_date)])
weekly_summary <- Counter_summary_by_date %>%
mutate(
Week = floor_date(Date, unit = "week", week_start = 1)
) %>%
group_by(CounterID, Week) %>%
summarise(
WeeklyTotal = sum(sum_by_date),
.groups = 'drop'
)
monthly_summary <- Counter_summary_by_date %>%
mutate(
month = floor_date(Date, unit = "month")
) %>%
group_by(CounterID, month) %>%
summarise(
MonthlyTotal = sum(sum_by_date),
.groups = 'drop'
)
# Export existing summaries
ecocounter_no_NA <- final_data %>% na.exclude()
write_csv(ecocounter_no_NA, file = "Data/Processed/Eco_Counter_no_NA.csv")
write_csv(max_date, file = "Data/Processed/Max_Daily.csv")
write_csv(weekly_summary, file = "Data/Processed/Max_Weekly.csv")
write_csv(monthly_summary, file = "Data/Processed/Max_Monthly.csv")
cat("\nProcessing complete! All files ready for Power BI auto-refresh.\n")
cat("Power BI files created in: Data/Processed/PowerBI/\n")
cat("Data for", length(unique(processed_data$CounterID)), "sites processed successfully.\n")
cat("\n=== IMPORTANT NOTES ===\n")
cat("• TotalCount uses OUT pedestrian count only (avoids double counting)\n")
cat("• CombinedCount available for reference (IN + OUT)\n")
cat("• Power BI will auto-refresh when you re-run this script\n")
cat("• Check PowerBI_Refresh_Log.csv for update history\n")
