---
title: "powerBI"
output: html_document
date: "2025-06-16"
editor_options: 
  chunk_output_type: console
---
```{r}
# Load required libraries
library(dplyr)
library(lubridate)
library(readr)

# Set file path at the top
file_path <- "C:/Users/tkatayama/OneDrive - DOI/Documents/Projects/R/People_Counter/Data/Raw/All_20250507_0615.csv"

# Function to process eco-counter data for multiple sites
process_ecocounter_data <- function(file_path) {
  # Read CSV file, skipping first 2 rows
  raw_data <- read_csv(file_path, skip = 2, na = c(""))
 
  # Display original column names for verification
  cat("Original column names:\n")
  print(names(raw_data))
 
  # First, clean and prepare the base data
  clean_data <- raw_data %>%
    filter(!is.na(Time)) %>%
    mutate(
      DateTime = mdy_hm(Time),
      DateTime = case_when(
        is.na(DateTime) ~ mdy_hms(paste0(Time, ":00")),
        TRUE ~ DateTime
      )
    ) %>%
    filter(!is.na(DateTime)) %>%
    mutate(
      Date = as.Date(DateTime),
      Hour = hour(DateTime),
      Year = year(DateTime),
      Month = month(DateTime),
      Day = day(DateTime),
      Weekday = wday(DateTime, label = TRUE)
    )
 
  # Create separate dataframes for each site
  lighthouse_data <- clean_data %>%
    select(DateTime, Date, Hour, Year, Month, Day, Weekday,
           PedestriansIn = `Lighthouse Pedestrian towards lighthouse`,
           PedestriansOut = `Lighthouse Pedestrian out of lighthouse`) %>%
    mutate(CounterID = "Lighthouse")
 
  bayside_data <- clean_data %>%
    select(DateTime, Date, Hour, Year, Month, Day, Weekday,
           PedestriansIn = `Bayside Pedestrian In`,
           PedestriansOut = `Bayside Pedestrian out`) %>%
    mutate(CounterID = "Bayside Trail")
 
  lot1_data <- clean_data %>%
    select(DateTime, Date, Hour, Year, Month, Day, Weekday,
           PedestriansIn = `CABR Lot 1 Counter IN`,
           PedestriansOut = `CABR Lot 1 Counter OUT`) %>%
    mutate(CounterID = "Tidepool Lot 1")
 
  lot2_data <- clean_data %>%
    select(DateTime, Date, Hour, Year, Month, Day, Weekday,
           PedestriansIn = `CABR Lot 2 Counter IN`,
           PedestriansOut = `CABR Lot 2 Counter OUT`) %>%
    mutate(CounterID = "Tidepool Lot 2")
 
  Oceanside1_data <- clean_data %>%
    select(DateTime, Date, Hour, Year, Month, Day, Weekday,
           PedestriansIn = `Oceanside Trail 1 Pedestrian IN`,
           PedestriansOut = `Oceanside Trail 1 Pedestrian OUT`) %>%
    mutate(CounterID = "Oceanside Trail 1")
 
  # Combine all sites
  processed_data <- bind_rows(lighthouse_data, bayside_data, lot1_data,
                              lot2_data, Oceanside1_data) %>%
    mutate(
      TotalCount = case_when(
        is.na(PedestriansIn) & is.na(PedestriansOut) ~ NA_real_,
        is.na(PedestriansIn) ~ PedestriansOut,
        is.na(PedestriansOut) ~ PedestriansIn,
        TRUE ~ PedestriansIn + PedestriansOut
      )
    ) %>%
    select(
      CounterID,
      DateTime,
      Date,
      Hour,
      Year,
      Month,
      Day,
      Weekday,
      PedestriansIn,
      PedestriansOut,
      TotalCount
    ) %>%
    arrange(CounterID, DateTime)
 
  return(processed_data)
}

# Function to create Power BI optimized datasets
create_powerbi_datasets <- function(data) {
 
  # 1. Main dataset - cleaned and formatted for Power BI
  powerbi_main <- data %>%
    filter(!is.na(TotalCount)) %>%
    mutate(
      # Create Power BI friendly date formats
      DateKey = as.numeric(format(Date, "%Y%m%d")),
      MonthYear = format(Date, "%Y-%m"),
      MonthName = month(Date, label = TRUE, abbr = FALSE),
      DayOfWeek = wday(Date, label = TRUE, abbr = FALSE),
      Quarter = paste0("Q", quarter(Date)),
      WeekNumber = week(Date),
      IsWeekend = ifelse(wday(Date) %in% c(1, 7), "Weekend", "Weekday"),
      TimeOfDay = case_when(
        Hour >= 5 & Hour < 12 ~ "Morning",
        Hour >= 12 & Hour < 17 ~ "Afternoon",
        Hour >= 17 & Hour < 21 ~ "Evening",
        TRUE ~ "Night"
      ),
      # Create descriptive trail categories
      TrailCategory = case_when(
        grepl("Lighthouse", CounterID) ~ "Lighthouse Area",
        grepl("Bayside", CounterID) ~ "Bayside Trail",
        grepl("Tidepool", CounterID) ~ "Tidepool Area",
        grepl("Oceanside", CounterID) ~ "Oceanside Trail",
        TRUE ~ "Other"
      )
    ) %>%
    # Ensure no missing values in key fields
    filter(!is.na(PedestriansIn) | !is.na(PedestriansOut)) %>%
    # Replace NA with 0 for calculations
    mutate(
      PedestriansIn = ifelse(is.na(PedestriansIn), 0, PedestriansIn),
      PedestriansOut = ifelse(is.na(PedestriansOut), 0, PedestriansOut),
      TotalCount = PedestriansIn + PedestriansOut
    )
 
  # 2. Daily summary for dashboard KPIs
  daily_summary <- powerbi_main %>%
    group_by(CounterID, TrailCategory, Date, DateKey, MonthYear,
             MonthName, DayOfWeek, IsWeekend, Quarter) %>%
    summarise(
      DailyTotal = sum(TotalCount, na.rm = TRUE),
      DailyIn = sum(PedestriansIn, na.rm = TRUE),
      DailyOut = sum(PedestriansOut, na.rm = TRUE),
      PeakHour = Hour[which.max(TotalCount)],
      PeakHourCount = max(TotalCount, na.rm = TRUE),
      ActiveHours = sum(TotalCount > 0, na.rm = TRUE),
      .groups = 'drop'
    )
 
  # 3. Monthly summary for trend analysis
  monthly_summary <- powerbi_main %>%
    group_by(CounterID, TrailCategory, MonthYear, Year, Month, MonthName, Quarter) %>%
    summarise(
      MonthlyTotal = sum(TotalCount, na.rm = TRUE),
      MonthlyAvgDaily = round(mean(TotalCount, na.rm = TRUE), 1),
      MonthlyPeak = max(TotalCount, na.rm = TRUE),
      DaysActive = n_distinct(Date),
      .groups = 'drop'
    ) %>%
    arrange(CounterID, Year, Month)
 
  # 4. Hour of day patterns
  hourly_patterns <- powerbi_main %>%
    group_by(CounterID, TrailCategory, Hour, TimeOfDay) %>%
    summarise(
      AvgHourlyCount = round(mean(TotalCount, na.rm = TRUE), 1),
      MedianHourlyCount = round(median(TotalCount, na.rm = TRUE), 1),
      MaxHourlyCount = max(TotalCount, na.rm = TRUE),
      TotalObservations = n(),
      .groups = 'drop'
    )
 
  # 5. Trail comparison summary
  trail_comparison <- powerbi_main %>%
    group_by(CounterID, TrailCategory) %>%
    summarise(
      TotalVisitors = sum(TotalCount, na.rm = TRUE),
      AvgDailyVisitors = round(mean(TotalCount, na.rm = TRUE), 1),
      PeakSingleHour = max(TotalCount, na.rm = TRUE),
      DaysOfData = n_distinct(Date),
      FirstRecordDate = min(Date),
      LastRecordDate = max(Date),
      .groups = 'drop'
    ) %>%
    arrange(desc(TotalVisitors))
 
  # Return list of datasets
  return(list(
    main = powerbi_main,
    daily = daily_summary,
    monthly = monthly_summary,
    hourly = hourly_patterns,
    comparison = trail_comparison
  ))
}

# Function to export Power BI datasets
export_powerbi_data <- function(datasets, base_path = "Data/Processed/PowerBI/") {
 
  # Create directory if it doesn't exist
  if (!dir.exists(base_path)) {
    dir.create(base_path, recursive = TRUE)
  }
 
  # Export each dataset
  write_csv(datasets$main, paste0(base_path, "EcoCounter_PowerBI_Main.csv"))
  write_csv(datasets$daily, paste0(base_path, "EcoCounter_PowerBI_Daily.csv"))
  write_csv(datasets$monthly, paste0(base_path, "EcoCounter_PowerBI_Monthly.csv"))
  write_csv(datasets$hourly, paste0(base_path, "EcoCounter_PowerBI_Hourly.csv"))
  write_csv(datasets$comparison, paste0(base_path, "EcoCounter_PowerBI_Comparison.csv"))
 
  cat("\n=== POWER BI DATASETS EXPORTED ===\n")
  cat("Main dataset:", nrow(datasets$main), "records\n")
  cat("Daily summary:", nrow(datasets$daily), "records\n")
  cat("Monthly summary:", nrow(datasets$monthly), "records\n")
  cat("Hourly patterns:", nrow(datasets$hourly), "records\n")
  cat("Trail comparison:", nrow(datasets$comparison), "records\n")
  cat("\nFiles saved to:", base_path, "\n")
 
  # Print dataset preview
  cat("\n=== SAMPLE OF MAIN DATASET ===\n")
  print(head(datasets$main, 5))
 
  return(datasets)
}

# Main execution
processed_data <- process_ecocounter_data(file_path)

# Create Power BI optimized datasets
powerbi_datasets <- create_powerbi_datasets(processed_data)

# Export for Power BI
final_powerbi_data <- export_powerbi_data(powerbi_datasets)

# Keep your existing exports
final_data <- export_for_arcgis(processed_data)

# Create summary statistics (your existing code)
Counter_summary_by_date <- processed_data %>%
  na.exclude() %>%
  filter(Hour >= 9 & Hour <= 17) %>%
  group_by(CounterID, Date) %>%
  summarise(sum_by_date = sum(PedestriansOut))

max_date <- Counter_summary_by_date %>%
  group_by(CounterID) %>%
  summarise(max_value = max(sum_by_date),
            max_date = Date[which.max(sum_by_date)])

weekly_summary <- Counter_summary_by_date %>%
  mutate(
    Week = floor_date(Date, unit = "week", week_start = 1)
  ) %>%
  group_by(CounterID, Week) %>%
  summarise(
    WeeklyTotal = sum(sum_by_date),
    .groups = 'drop'
  )

monthly_summary <- Counter_summary_by_date %>%
  mutate(
    month = floor_date(Date, unit = "month")
  ) %>%
  group_by(CounterID, month) %>%
  summarise(
    MonthlyTotal = sum(sum_by_date),
    .groups = 'drop'
  )

# Export existing summaries
ecocounter_no_NA <- final_data %>% na.exclude()
#write_csv(ecocounter_no_NA, file = "Data/Processed/Eco_Counter_no_NA.csv")
write_csv(max_date, file = "Data/Processed/Max_Daily.csv")
write_csv(weekly_summary, file = "Data/Processed/Max_Weekly.csv")
write_csv(monthly_summary, file = "Data/Processed/Max_Monthly.csv")

cat("\nProcessing complete! All files ready for Power BI and ArcGIS Online upload.\n")
cat("Power BI files created in: Data/Processed/PowerBI/\n")
cat("Data for", length(unique(processed_data$CounterID)), "sites processed successfully.\n")
```

